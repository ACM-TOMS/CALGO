% Skim 5/15/07
\section{Solving POPs with multiple optimal solutions}
\label{multiple}

Consider solving the problem of minimizing the generalized RosenBrock function with $n=40$ 
over the entire space by issuing the commands 
\begin{verbatim}
>> param.relaxOrder = 2; 
>> sparsePOP('Rosenbrock(40,0)',param);
\end{verbatim}
The output data is 
\begin{verbatim}
  . . . . . . . . . .

# Approximate optimal value information:
  SDPobjValue         = +1.0000022e+00
  POP.objValue         = +1.0099885e+02
  relative obj error  = +9.901e-01
  POP.absError        = +0.000e+00
  POP.scaledError     = +0.000e+00
  . . . . . . . . . .
  
\end{verbatim}
% Skim 5/15/07
A big difference between {\sf SDPobjValue} and {\sf POP.objValue} indicates that 
no accurate approximate optimal solution has been found. This is because  {\bf SparsePOP can not 
handle efficiently
a POP having multiple optimal solutions}. One way to resolve this difficulty is to add either 
$x_1 \leq 0$ or $x_1 \geq 0$ as an inequality constraint to select only one optimal solution. 
This is actually shown in the previous subsection.
% Skim 5/15/07 
Such inequality constraints, however, can  not be known before attaining an approximate 
optimal solution. This difficulty can be better handled by linear perturbation to the objective function of a given POP 
that possesses possibly multiple solutions. Let $\epsilon > 0$ be a small positive number such as $1.0$e-$4$, 
and $\p \in \Real^n$ be a column vector whose elements $p_i$ $(i=1,2,\ldots,n)$ are chosen 
randomly from the unit interval $[0,1]$. The objective function $f_0(\x)$ of a given 
POP is then replaced by a perturbed objective function $f_0(\x) + \epsilon \p^T\x$ such that the resulting POP has 
a unique optimal solution. 
% Skim 5/15/07
The optimal solution of the perturbed POP is expected to be an approximation  for an optimal solution of the original POP. 
We refer to \cite{WAKI04} for more details. 
In the minimization of the generalized Rosenbrock function, this perturbation technique is carried out 
as follows: 
\begin{verbatim}
>> param.relaxOrder = 2; 
>> param.perturbation = 1.0e-4; 
>> sparsePOP('Rosenbrock(40,0)',param);
\end{verbatim}
The result is: 
\begin{verbatim}
  . . . . . . . . . .

# Approximate optimal value information:
  SDPobjValue         = +9.9991951e-01
  POP.objValue         = +1.0001360e+00
  relative obj error  = +2.164e-04
  POP.absError        = +0.000e+00
  POP.scaledError     = +0.000e+00
  . . . . . . . . . .
  
\end{verbatim}
Once we obtain an approximate solution $\hat{x}$ of a POP  by applying the 
perturbation technique, we can apply sparsePOP.m again to the original POP with updated 
lower and upper constraints 
such that 
\begin{eqnarray*}
\mbox{lbd}_i & = & \max\{ \mbox{lbd}_i, \ \hat{x}_i - \delta_i \} \ (i=1,2,\ldots,n), \\ 
\mbox{ubd}_i & = & \min\{ \mbox{ubd}_i, \ \hat{x}_i + \delta_i \} \ (i=1,2,\ldots,n). 
\end{eqnarray*}
for some small positive numbers $\delta_i$ $(i=1,2,\ldots,n)$. If the new lower and 
upper bounds separate a single optimal solution from all other optimal solutions of 
the original POP, this method is expected to work effectively. 
