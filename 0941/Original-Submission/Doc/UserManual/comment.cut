\appendix

\newpage
\section{List of {\sc Matlab} functions} \label{sec:functionality}

Tables \ref{tab:htucker_functions1}, \ref{tab:htucker_functions2}, and~\ref{tab:htucker_functions3}
give an overview of the complete functionality of our {\sc Matlab} toolbox \htucker. More details
for the use of each function can be obtained using the command {\tt help}.

\begin{table}[h] \small
  \begin{tabular}{|p{3.2cm}|p{11cm}|}
\hline
\multicolumn{2}{|c|}{Construction of \texttt{htensor} objects.} \\
\hline
\texttt{htensor}            & Construct a tensor in HTD and return {\tt htensor} object.\\
\texttt{define\_tree}       & Define dimension tree.\\
\hline \hline
\multicolumn{2}{|c|}{Basic functionality} \\
\hline
\texttt{cat}                & Concatenate two {\tt htensor} objects.\\
\texttt{change\_dimtree}    & Change dimension tree of htensor.\\
\texttt{change\_root}       & Change root of the dimension tree.\\
\texttt{check\_htensor}     & Check internal consistency of {\tt htensor}.\\
\texttt{conj}               & Complex conjugate of {\tt htensor}.\\
\texttt{disp}               & Command window display of dimension tree of {\tt htensor}.\\
\texttt{display}            & Command window display of dimension tree of {\tt htensor}.\\
\texttt{disp\_all}          & Command window display of {\tt htensor}.\\
\texttt{end}                & Last index in one mode of {\tt htensor}.\\
\texttt{equal\_dimtree}     & Compare dimension trees of two {\tt htensor} objects.\\
\texttt{full}               & Convert {\tt htensor} to a (full) tensor.\\
\texttt{full\_block}        & Return subblock of {\tt htensor} as a (full) tensor.\\
\texttt{full\_leaves}        & Convert leaf matrices $U_t$ to dense matrices. \\
\texttt{ipermute}           & Inverse permute dimensions of {\tt htensor}.\\
\texttt{isequal}            & Check whether two htensors are equal.\\
\texttt{mrdivide (/)}       & Scalar division for {\tt htensor}.\\
\texttt{mtimes (*)}         & Scalar multiplication for {\tt htensor}.\\
\texttt{ndims}              & Order (number of dimensions) of {\tt htensor}.\\
\texttt{ndofs}              & Number of degrees of freedom in {\tt htensor}.\\
\texttt{norm}               & Norm of {\tt htensor}.\\
\texttt{norm\_diff}         & Norm of difference between {\tt htensor} and full tensor.\\
\texttt{nvecs}              & Dominant left singular vectors for matricization of {\tt htensor}.\\
\texttt{permute}            & Permute dimensions of {\tt htensor}.\\
\texttt{plot\_sv}           & Plot singular value tree of {\tt htensor}.\\
\texttt{rank}               & Hierarchical ranks of {\tt htensor}.\\
\texttt{singular\_values}   & Singular values for matricizations of {\tt htensor}.\\
\texttt{size}               & Size of {\tt htensor}.\\
\texttt{sparse\_leaves}      & Convert leaf matrices $U_t$ to sparse matrices. \\
\texttt{spy}                & Plot sparsity pattern of the nodes of {\tt htensor}.\\
\texttt{squeeze}            & Remove singleton dimensions from {\tt htensor}.\\
\texttt{subsasgn}           & Subscripted assignment for {\tt htensor}.\\
\texttt{subsref}            & Subscripted reference for {\tt htensor}.\\
\texttt{subtree}            & Return all nodes in the subtree of a node.\\
\texttt{uminus}             & Unary minus (-) of {\tt htensor}.\\
\texttt{uplus}              & Unary plus for {\tt htensor}.\\
\hline
\end{tabular}
  \caption{List of functions in \htucker{} toolbox (part 1).} \label{tab:htucker_functions1}
\end{table}

\begin{table} \small
  \begin{tabular}{|p{3.2cm}|p{11cm}|}
\hline
\multicolumn{2}{|c|}{Operations with \texttt{htensor} objects.} \\
\hline
\texttt{elem\_mult}         & Approximate element-by-element multiplication for {\tt htensor}.\\
\texttt{innerprod}          & Inner product for {\tt htensor}.\\
\texttt{minus (-)}          & Binary subtraction for {\tt htensor}.\\
\texttt{plus (+)}           & Binary addition for {\tt htensor}.\\
\texttt{power (.\^{}2)}     & Element-by-element square for {\tt htensor}.\\
\texttt{times (.*)}         & Element-by-element multiplication for {\tt htensor}.\\
\texttt{ttm}                & $N$-mode multiplication of {\tt htensor} with matrix.\\
\texttt{ttt}                & Tensor-times-tensor for {\tt htensor}.\\
\texttt{ttv}                & Tensor-times-vector for {\tt htensor}.\\
\hline \hline
\multicolumn{2}{|c|}{Orthogonalization and truncation.} \\
\hline
\texttt{gramians}           & Reduced Gramians of {\tt htensor} in orthogonalized HTD.\\
\texttt{gramians\_cp}       & Reduced Gramians of CP tensor.\\
\texttt{gramians\_nonorthog} & Reduced Gramians of {\tt htensor}.\\
\texttt{gramians\_sum}      & Reduced Gramians for sum of {\tt htensor} objects.\\
\texttt{left\_svd\_gramian} & Left singular vectors and values from Gramian.\\
\texttt{left\_svd\_qr}      & Left singular vectors and values.\\
\texttt{orthog}             & Orthogonalize HTD of {\tt htensor}.\\
\texttt{trunc\_rank}        & Return rank according to user-specified parameters.\\
\texttt{truncate}            & Truncate full tensor/{\tt htensor}/CP to {\tt htensor}.\\
\texttt{truncate\_cp}       & Truncate CP tensor to lower-rank {\tt htensor}. \\
\texttt{truncate\_ltr}      & Truncate full tensor to {\tt htensor}, leaves-to-root.\\
\texttt{truncate\_nonorthog} & Truncate {\tt htensor} to lower-rank {\tt htensor}.\\
\texttt{truncate\_rtl}      & Truncate full tensor to {\tt htensor}, root-to-leaves.\\
\texttt{truncate\_std}       & Truncate {\tt htensor} to lower-rank {\tt htensor}.\\
\texttt{truncate\_sum}      & Truncate sum of {\tt htensor} objects to lower-rank {\tt htensor}.
.\\
\hline \hline
\multicolumn{2}{|c|}{Linear Operators.} \\
\hline
\texttt{apply\_mat\_to\_mat}& Applies an operator in HTD to another operator in HTD. \\
\texttt{apply\_mat\_to\_vec}& Applies an operator in HTD to {\tt htensor}.\\
\texttt{full\_mat}          & Full matrix represented by an operator in HTD.\\
\texttt{innerprod\_mat}     & Weighted inner product for {\tt htensor}.\\
\hline \hline
\multicolumn{2}{|c|}{Interface with Tensor Toolbox.} \\
\hline
\texttt{ktensor\_approx}    & Approximation of {\tt htensor} by {\tt ktensor}.\\
\texttt{mttkrp}             & Building block for approximating {\tt htensor} by {\tt ktensor}.\\
\texttt{ttensor}            & Convert {\tt htensor} to a Tensor Toolbox ttensor.\\
\hline
\end{tabular}
  \caption{List of functions in \htucker{} toolbox (part 2).} \label{tab:htucker_functions2}
\end{table}

\begin{table} \small
\begin{tabular}{|p{4.4cm}|p{9.8cm}|}
 \hline
\multicolumn{2}{|c|}{Auxiliary functions for full tensors.} \\
\hline
\texttt{dematricize}        & Determine (full) tensor from matricization.\\
\texttt{diag3d}             & Return third-order diagonal tensor.\\
\texttt{isindexvector}      & Check whether input is index vector.\\
\texttt{khatrirao\_aux}     & Khatri-Rao product.\\
\texttt{khatrirao\_t}       & Transposed Khatri-Rao product.\\
\texttt{matricize}          & Matricization of (full) tensor.\\
\texttt{spy3}               & Plot sparsity pattern of order-$3$ tensor.\\
\texttt{ttm}                & $N$-mode multiplication of (full) tensor with matrix.\\
\texttt{ttt}                & Tensor times tensor (full tensors).\\
%\hline \hline
%\multicolumn{2}{|c|}{Test.} \\
%\hline
%\texttt{test\_functions}           & Simple test script to check for obvious bugs.\\
\hline \hline
\multicolumn{2}{|c|}{Example tensors.} \\
\hline
\texttt{gen\_invlaplace}     & htensor for approx. inverse of Laplace-like matrix.\\
\texttt{gen\_laplace}        & {\tt htensor} for Laplace-like matrix.\\
\texttt{gen\_sin\_cos}       & Function-valued {\tt htensor} for sine and cosine.\\
\texttt{htenones}           & {\tt htensor} with all elements one.\\
\texttt{htenrandn}          & Random {\tt htensor}.\\
\texttt{laplace\_core}       & Core tensor for Laplace operator.\\
\texttt{reciproc\_sum}             & Function-valued tensor for $1/(\xi_1 + \cdots + \xi_d)$.\\
\hline \hline
\multicolumn{2}{|c|}{Examples} \\
\hline
\texttt{cg\_tensor}             & Truncated Conjugate Gradient method for htensor.\\
\texttt{demo\_basics}           & Demonstration of basic htensor functionality.\\
\texttt{demo\_constructor}      & Demonstration of htensor constructors.\\
\texttt{demo\_elem\_reciprocal} & Demonstration of element-wise reciprocal.\\
\texttt{demo\_function}         & Demonstration of htensor function approximation.\\
\texttt{demo\_invlaplace}       & Demonstration of approximate inverse Laplace.\\
\texttt{demo\_operator}         & Demonstration of operator-HTD format.\\
\texttt{elem\_reciprocal}       & Iterative computation of elementwise reciprocal for htensor.\\
\texttt{example\_cancellation}  & Cancellation in $\tan(x) + 1/x - \tan(x)$.\\
\texttt{example\_cancellation2} & Cancellation in $\exp(-x^2) + \sin(x)^2 + \cos(x)^2$.\\
\texttt{example\_cookies}       & Apply CG method to a parametric PDE.\\
\texttt{example\_maxest}        & Example for computing element of maximal absolute value.\\
\texttt{example\_spins}         & Demonstration of operator-HTD for 1D spin system.\\
\texttt{example\_sum\_reciproc} & Apply element-wise reciprocal method to sum of tensors.\\
\texttt{example\_truncation}    & Comparison of speed for different truncation methods.\\
\texttt{handle\_inv\_mat}       & Function handle to Kronecker structured matrix multiplication.\\
\texttt{handle\_lin\_mat}       & Function handle to Kronecker structured matrix multiplication.\\
\texttt{maxest}                 & Approximate element of maximal absolute value.\\
\hline
\end{tabular}
  \caption{List of functions in \htucker{} toolbox (part 3).} \label{tab:htucker_functions3}
%\caption{List of examples in \htucker{} toolbox.} \label{tab:htucker_functions3}
\end{table}


\newpage
\section{Proofs of approximation results} \label{sec:proofs}

In this appendix, we provide and prove bounds on the approximation
error of the leaves-to-root method (Section~\ref{sec:LtR}) and of
truncation in HTD without initial orthogonalization (Section~\ref{sec:trunc_var}). The proofs will be based on the following basic result.
\begin{lemma} \label{lem:nested_projections} Consider a dimension tree
  $\calT$ and orthogonal projections $\pi_t = W_t W_t^H$ for $t\in \calT$.
If the projections are nested:
  \[
  \text{span}(W_t) \subset \text{span}(W_{t_r} \otimes W_{t_l}) \qquad
  \text{ for all } t \in \calN(\calT),
  \]
  then all projections $\pi_s$ and $\pi_t$ commute.
\end{lemma}
\begin{proof}
Any two nodes $s,t\in \calT$ are either disjoint ($s \cap t =
  \emptyset$) or nested (w.l.o.g., $s \subset t$):
\begin{enumerate}
\item $s \cap t = \emptyset$: In this case, the statement of the lemma follows directly from \[W_s
  W_s^H \circ_s W_t W_t^H \circ_t \calX = W_t W_t^H \circ_t W_s W_s^H
  \circ_s \calX,\] for any tensor $\calX$.
\item $s \subset t$: Without loss of generality, we may assume that the
  modes contained in node $s$ are the leading modes of $t$. Then $\text{span}(W_t) \subset
  \text{span}(I \otimes W_s)$ and the statement is an immediate consequence of the
  obvious fact that projections onto nested subspaces commute.
\end{enumerate}
\end{proof}
%
\noindent To simplify the presentation, we require some additional notation. All truncation methods
involve a sequence of projections to some subspaces $\calR(W_t)$ for nodes
$t \in \calT$. Note that there is no projection associated with the root node.
Moreover, the truncations for both children $t_l, t_r$ of the root node arise from
essentially the same singular value decomposition, as $X^{(t_l)} = (X^{(t_r)})^T$. Thus, it is sufficient
to consider only one of them for the error
bound. We therefore define the set $\calT'$ to
contain all nodes of $\calT$, except for the root node and one of its
children.

The leaves-to-root method described in Algorithm~\ref{alg:trunc_LtR}
can be written recursively in terms of projections as
  \[
  \tilde{\calX}_L := \calX, \quad
  \tilde{\calX}_{\ell-1} = \prod_{t \in \calT'_\ell}
  \pi_t \tilde{\calX}_\ell, \quad
  \tilde{\calX} := \tilde{\calX}_0,
  \]
with $\calT_L' = \calL(\calT) \cap \calT'$ and $\calT_\ell' = (\calT_\ell \cap \calT') \setminus \calL(\calT)$, $\ell = 1, \ldots, L-1$.
Defining the matrix $W_t$ to contain the $k_t$ dominant left singular vectors of $\tilde{X}_{\ell}^{(t)}$ for $t \in \calT_{\ell}'$
and $\ell \in\{ 1,\ldots,L\}$, we set $\pi_t = W_t W_t^H$.
%
\begin{lemma} \label{lem:err_LtR} The truncation error of the
  leaves-to-root method described in Algorithm~\ref{alg:trunc_LtR} satisfies
  \[
  \norma{ \calX - \tilde \calX }_2
  \le \sqrt{ \sum_{\ell = 1}^L \sum_{t \in \calT_\ell'}
    \delta_{k_t}(\tilde{X}_\ell^{(t)})^2}
  \le \sqrt{2d-3} \: \norm{ \calX - \calX_{\text{best}} }_2,
\]
where $\calX_{\text{best}}$ is a best approximation of
$\calX$ in $\calH$-Tucker$((k_t)_{t \in \calT})$, and the low-rank approximation error $\delta_{k_t}$
is defined as in~(\ref{eq:delta}).
%
\end{lemma}
\begin{proof}
  We expand $\calX - \tilde{\calX}$ into a sum:
\[
\calX - \tilde{\calX} = \sum_{\ell=1}^L
  \tilde{\calX}_\ell - \tilde{\calX}_{\ell-1}
= \sum_{\ell=1}^L (I - \pi_\ell) \pi_{\ell+1} \cdots \pi_L \calX,
\quad \text{ with }\pi_\ell := \prod_{t \in \calT'_\ell} \pi_t.
\]
 As
 the projections $\pi_t$ commute, each summand is orthogonal to all subsequent summands and therefore
$\| \calX - \tilde{\calX} \|_2^2
= \sum_{\ell=1}^L \| \tilde{\calX}_\ell - \pi_\ell \tilde{\calX}_\ell\|_2^2$.
A similar reasoning for the projections on
 level $\ell$ leads to
$\| \tilde{\calX}_\ell - \pi_\ell \tilde{\calX}_\ell \|_2^2
\le \sum_{t \in \calT_\ell'}
\| \tilde{\calX}_\ell - \pi_t \tilde{\calX}_\ell \|_2^2$,
showing the first inequality of the lemma.

 For the second inequality, we will prove that $\|\tilde{\calX}_\ell -
 \pi_t \tilde{\calX}_\ell \|_2 \le \|\calX - \calX_{\text{best}}\|_2$ for all
 nodes $t$. We start by defining $\calX_{t,\text{best}}$ to be a best
 approximation of $\calX$ under the condition that the rank of the $t$-matricization is not larger than
 $k_t$. Clearly, $\normb{ \calX - \calX_{t,\text{best}} }_2 \le \normb{
   \calX - \calX_{\text{best}} }_2$. Furthermore, we define the set
\[
    \calS_t := \big\{ \calY \in \C^{n_1 \times \cdots \times n_d} \big| \rank
    (Y^{(t)}) \le k_t \text{ and } \text{span}(Y^{(s)}) \subset
    \text{span}(W_s) \ \forall s \in \bigcup_{j=\ell+1}^L \calT'_j \big\}.
\]
Note that $\pi_t
\tilde{\calX}_\ell$ is a minimizer of $\|\tilde{\calX}_\ell - \calY\|_2$
on $\calS_t$, as $\pi_t$ is based on the SVD of
$\tilde{X}_\ell^{(t)}$. Furthermore, $\pi_{\ell+1} \cdots \pi_L
\calX_{t,\text{best}}$ is a member of $\calS_t$ (from~\cite[Lemma
  3.15]{Gra10}). In conclusion,
  \[
  \normb{ \tilde{\calX}_\ell - \pi_t \tilde{\calX}_\ell}_2
  \le \normb{ \pi_{\ell+1} \cdots \pi_L \calX - \pi_{\ell+1} \cdots \pi_L
  \calX_{t,\text{best}} }_2
  \le \normb{ \calX - \calX_{t,\text{best}} }_2
  \le \normb{ \calX - \calX_{\text{best}} }_2.
  \]
\end{proof}
%

\noindent Truncation in HTD without initial orthogonalization
is described in Algorithm~\ref{alg:trunc_htucker_2} and can also be written recursively in terms
of projections: $\tilde{\calX} := \prod_{t \in \calT'} \pi_t \calX$, where $\pi_t$ is the orthogonal projection onto the subspace spanned by the $k_t$ dominant left singular vectors of
\begin{equation} \label{eq:defblabla}
\tilde{U}_t V_t^H = \Big( \prod_{s \in \calT', s \subsetneq t} \pi_s \Big) X^{(t)} =: \tilde{X}^{(t)}_t.
\end{equation}
Thus, we can equivalently define $\tilde{\calX} = \tilde{\calX}_{t_\text{root}}$.
%
\begin{lemma} \label{lem:err_AddTrunc} The truncation error of HTD without initial orthogonalization
described in Algorithm~\ref{alg:trunc_htucker_2}
satisfies:
  \[
  \norma{ \calX - \tilde \calX }_2
  \: \le \: \sqrt{ \sum_{t \in \calT'}
    \delta_{k_t}(\tilde{X}_t^{(t)})^2}
  \: \le \: \sqrt{2d - 3} \:
  \norma{ \calX - \calX_{\text{best}} }_2,
  \]
  where $\calX_{\text{best}}$ is a best approximation of $\calX$ in
  $\calH$-Tucker$((k_t)_{t \in \calT})$.
%
\end{lemma}
\begin{proof}
  We define the projections $\tilde{\pi}_t$ by the recursion
  \[
  \tilde{\pi}_t = \left\{ \begin{array}{ll}
    \pi_t & \text{if $t$ is a leaf node,}\\
    \tilde{\pi}_{t_l} \tilde{\pi}_{t_r} \pi_t & \text{otherwise,}
    \end{array} \right.
  \]
  where we have formally set $\pi_{t_\text{root}}$ to the identity.
Note that the projections $\tilde{\pi}_t$ commute.
  Let us now consider $\tilde{\calX}_t = \tilde{\pi}_{t_l} \tilde{\pi}_{t_r} \calX$
  for a non-leaf node $t$:
  \begin{align*}
    \| \calX - \tilde{\pi}_t \calX \|_2^2 &= \| \calX - \tilde{\pi}_{t_l} \calX + \tilde{\pi}_{t_l} \calX - \tilde{\pi}_{t_l} \tilde{\pi}_{t_r} \calX + \tilde{\pi}_{t_l} \tilde{\pi}_{t_r} \calX - \pi_t \tilde{\pi}_{t_l} \tilde{\pi}_{t_r} \calX \|_2^2 \\
    &\le \| \calX - \tilde{\pi}_{t_l} \calX\|_2^2
    + \|\calX - \tilde{\pi}_{t_r} \calX\|_2^2
    + \| \tilde{\calX}_t - \pi_t \tilde{\calX}_t \|_2^2.
  \end{align*}
Successive application of this inequality shows the first inequality of the lemma:
\[
\| \calX - \tilde{\calX}  \|_2^2
= \| \calX - \tilde{\pi}_{t_\text{root}} \calX \|_2^2
\le \sum_{t \in \calT'}
  \| \tilde{\calX}_t - \pi_t \tilde{\calX}_t \|_2^2 .
\]
For the second inequality, the proof is analogous to the one of Lemma~\ref{lem:err_LtR}.
% KOMMT IN DIE DISS, NICHT VERGESSEN: , replacing $\tilde{\calX}_\ell$ by $\tilde{\calX}_t$, $\bar{\pi}_{\ell+1}$ by $\tilde{\pi}_{t_l} \tilde{\pi}_{t_r}$ and $\calT'_{\ell+1}$ by $\{s \in \calT : s \subsetneq t \}$, the set of all descendants of $t$.
\end{proof}
%
